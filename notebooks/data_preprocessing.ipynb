{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf7d90c",
   "metadata": {
    "id": "fbf7d90c"
   },
   "source": [
    "If use Colab run cells markdown required by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ebbc5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22960,
     "status": "ok",
     "timestamp": 1717523637413,
     "user": {
      "displayName": "Mateusz Kołakowski",
      "userId": "17762159913948122325"
     },
     "user_tz": -120
    },
    "id": "a18ebbc5",
    "outputId": "19f5bd36-7757-4a50-fabf-9e760b109564"
   },
   "outputs": [],
   "source": [
    "# Colab required\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc59baa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 609,
     "status": "ok",
     "timestamp": 1717523652140,
     "user": {
      "displayName": "Mateusz Kołakowski",
      "userId": "17762159913948122325"
     },
     "user_tz": -120
    },
    "id": "1fc59baa",
    "outputId": "af89ad06-7811-4ae8-afbc-1aeb375c8b04"
   },
   "outputs": [],
   "source": [
    "# Colab required\n",
    "# change direction to Early_Baseline_PatentMatch_Paragraph_Classification_Mateusz repo folder\n",
    "%cd path_to_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa83b1",
   "metadata": {
    "executionInfo": {
     "elapsed": 2026,
     "status": "ok",
     "timestamp": 1717523678361,
     "user": {
      "displayName": "Mateusz Kołakowski",
      "userId": "17762159913948122325"
     },
     "user_tz": -120
    },
    "id": "57aa83b1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import json\n",
    "\n",
    "# Load the training and testing datasets from parquet files.\n",
    "# Parquet is a columnar storage file format optimized for use with large datasets.\n",
    "train_df = pd.read_parquet('./data/train.parquet')\n",
    "test_df = pd.read_parquet('./data/test.parquet')\n",
    "\n",
    "# Select only the relevant columns from the datasets.\n",
    "# We are interested in 'text', 'text_b', and 'label' columns.\n",
    "train_df = train_df[['text', 'text_b', 'label']]\n",
    "test_df = test_df[['text', 'text_b', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75469c",
   "metadata": {
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1717523686826,
     "user": {
      "displayName": "Mateusz Kołakowski",
      "userId": "17762159913948122325"
     },
     "user_tz": -120
    },
    "id": "bd75469c"
   },
   "outputs": [],
   "source": [
    "# Filter out rows where 'text' and 'text_b' are not strings or 'label' is not 0 or 1.\n",
    "# This ensures data integrity and that the data types are as expected.\n",
    "train_df_clean = train_df[\n",
    "    train_df['text'].apply(lambda x: isinstance(x, str)) &\n",
    "    train_df['text_b'].apply(lambda x: isinstance(x, str)) &\n",
    "    train_df['label'].isin([0, 1])\n",
    "]\n",
    "test_df_clean = test_df[\n",
    "    test_df['text'].apply(lambda x: isinstance(x, str)) &\n",
    "    test_df['text_b'].apply(lambda x: isinstance(x, str)) &\n",
    "    test_df['label'].isin([0, 1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbaf43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717523688542,
     "user": {
      "displayName": "Mateusz Kołakowski",
      "userId": "17762159913948122325"
     },
     "user_tz": -120
    },
    "id": "70dbaf43",
    "outputId": "340dd398-94d7-4755-c448-e1d86d0fa5e0"
   },
   "outputs": [],
   "source": [
    "# Print the number of records removed due to invalid data types.\n",
    "print(f'From train data removed {len(train_df) - len(train_df_clean)} invalid type records')\n",
    "print(f'From test data removed {len(test_df) - len(test_df_clean)} invalid type records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91183123",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1717523700034,
     "user": {
      "displayName": "Mateusz Kołakowski",
      "userId": "17762159913948122325"
     },
     "user_tz": -120
    },
    "id": "91183123",
    "outputId": "c74fb32b-33e9-452e-b383-a585b693128a"
   },
   "outputs": [],
   "source": [
    "# Add a 'source' column to each dataset to indicate whether the data is from training or testing.\n",
    "train_df_clean['source'] = 'train'\n",
    "test_df_clean['source'] = 'test'\n",
    "\n",
    "# Combine the filtered training and testing datasets into one DataFrame.\n",
    "combined_df = pd.concat([train_df_clean, test_df_clean])\n",
    "\n",
    "# Identify and handle duplicate data.\n",
    "# Create a temporary column 'sorted_pair' that contains a sorted tuple of 'text' and 'text_b'.\n",
    "# This helps in identifying duplicates regardless of the order of text pairs.\n",
    "combined_df['sorted_pair'] = combined_df.apply(\n",
    "    lambda row: tuple(sorted([row['text'], row['text_b']])), axis=1\n",
    ")\n",
    "\n",
    "# Group the data by 'sorted_pair' and filter out groups with more than one element.\n",
    "# These are potential duplicates.\n",
    "duplicate_groups = combined_df.groupby('sorted_pair').filter(lambda group: len(group) > 1)\n",
    "\n",
    "# Find records with contradictory labels within these duplicate groups.\n",
    "contradictory_labels = duplicate_groups.groupby('sorted_pair').filter(\n",
    "    lambda group: group['label'].nunique() > 1\n",
    ")\n",
    "print(f'Found {len(contradictory_labels)} duplicated records with contradictory labels')\n",
    "\n",
    "# Identify duplicate records that appear in both training and testing datasets.\n",
    "duplicates_across_sets = combined_df.groupby('sorted_pair')['source'].unique()\n",
    "duplicates_across_sets = duplicates_across_sets[duplicates_across_sets.apply(lambda x: len(x) > 1)]\n",
    "print(f'Found {len(duplicates_across_sets)} duplicate records between the training and test sets')\n",
    "\n",
    "# Remove duplicates within the training and testing datasets separately.\n",
    "# For the training dataset:\n",
    "train_df_clean['sorted_pair'] = train_df_clean.apply(\n",
    "    lambda row: tuple(sorted([row['text'], row['text_b']])), axis=1\n",
    ")\n",
    "train_df_clean = train_df_clean.drop_duplicates(subset='sorted_pair', keep='first')\n",
    "print(f'Removed duplicates from the training dataset.')\n",
    "\n",
    "# For the testing dataset:\n",
    "test_df_clean['sorted_pair'] = test_df_clean.apply(\n",
    "    lambda row: tuple(sorted([row['text'], row['text_b']])), axis=1\n",
    ")\n",
    "test_df_clean = test_df_clean.drop_duplicates(subset='sorted_pair', keep='first')\n",
    "print(f'Removed duplicates from the testing dataset.')\n",
    "\n",
    "# Convert the datasets to a list of tuples in the format (text, text_b, label).\n",
    "train_dataset = list(train_df_clean[['text', 'text_b', 'label']].itertuples(index=False, name=None))\n",
    "test_dataset = list(test_df_clean[['text', 'text_b', 'label']].itertuples(index=False, name=None))\n",
    "\n",
    "print(f'Final len of train dataset: {len(train_dataset)}')\n",
    "print(f'Final len of test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Save the datasets to JSON files.\n",
    "with open('./data/train_dataset.json', 'w') as file:\n",
    "    json.dump(train_dataset, file)\n",
    "\n",
    "with open('./data/test_dataset.json', 'w') as file:\n",
    "    json.dump(test_dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a444d",
   "metadata": {
    "id": "fa0a444d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
