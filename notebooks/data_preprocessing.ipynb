{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf7d90c",
   "metadata": {},
   "source": [
    "If use Colab run cells markdown required by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ebbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab required\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc59baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab required\n",
    "# change direction to Early_Baseline_PatentMatch_Paragraph_Classification_Mateusz repo folder\n",
    "%cd path_to_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57aa83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import json\n",
    "\n",
    "# Load the training and testing datasets from parquet files.\n",
    "# Parquet is a columnar storage file format optimized for use with large datasets.\n",
    "train_df = pd.read_parquet('../data/train.parquet')\n",
    "test_df = pd.read_parquet('../data/test.parquet')\n",
    "\n",
    "# Select only the relevant columns from the datasets.\n",
    "# We are interested in 'text', 'text_b', and 'label' columns.\n",
    "train_df = train_df[['text', 'text_b', 'label']]\n",
    "test_df = test_df[['text', 'text_b', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd75469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'text' and 'text_b' are not strings or 'label' is not 0 or 1.\n",
    "# This ensures data integrity and that the data types are as expected.\n",
    "train_df_clean = train_df[\n",
    "    train_df['text'].apply(lambda x: isinstance(x, str)) &\n",
    "    train_df['text_b'].apply(lambda x: isinstance(x, str)) &\n",
    "    train_df['label'].isin([0, 1])\n",
    "]\n",
    "test_df_clean = test_df[\n",
    "    test_df['text'].apply(lambda x: isinstance(x, str)) &\n",
    "    test_df['text_b'].apply(lambda x: isinstance(x, str)) &\n",
    "    test_df['label'].isin([0, 1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70dbaf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From train data removed 89 invalid type records\n",
      "From test data removed 25 invalid type records\n"
     ]
    }
   ],
   "source": [
    "# Print the number of records removed due to invalid data types.\n",
    "print(f'From train data removed {len(train_df) - len(train_df_clean)} invalid type records')\n",
    "print(f'From test data removed {len(test_df) - len(test_df_clean)} invalid type records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91183123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 duplicated records with contradictory labels\n",
      "Found 0 duplicate records between the training and test sets\n",
      "Removed duplicates from the training dataset.\n",
      "Removed duplicates from the testing dataset.\n",
      "Final len of train dataset: 2911\n",
      "Final len of test dataset: 776\n"
     ]
    }
   ],
   "source": [
    "# Add a 'source' column to each dataset to indicate whether the data is from training or testing.\n",
    "train_df_clean['source'] = 'train'\n",
    "test_df_clean['source'] = 'test'\n",
    "\n",
    "# Combine the filtered training and testing datasets into one DataFrame.\n",
    "combined_df = pd.concat([train_df_clean, test_df_clean])\n",
    "\n",
    "# Identify and handle duplicate data.\n",
    "# Create a temporary column 'sorted_pair' that contains a sorted tuple of 'text' and 'text_b'.\n",
    "# This helps in identifying duplicates regardless of the order of text pairs.\n",
    "combined_df['sorted_pair'] = combined_df.apply(\n",
    "    lambda row: tuple(sorted([row['text'], row['text_b']])), axis=1\n",
    ")\n",
    "\n",
    "# Group the data by 'sorted_pair' and filter out groups with more than one element.\n",
    "# These are potential duplicates.\n",
    "duplicate_groups = combined_df.groupby('sorted_pair').filter(lambda group: len(group) > 1)\n",
    "\n",
    "# Find records with contradictory labels within these duplicate groups.\n",
    "contradictory_labels = duplicate_groups.groupby('sorted_pair').filter(\n",
    "    lambda group: group['label'].nunique() > 1\n",
    ")\n",
    "print(f'Found {len(contradictory_labels)} duplicated records with contradictory labels')\n",
    "\n",
    "# Identify duplicate records that appear in both training and testing datasets.\n",
    "duplicates_across_sets = combined_df.groupby('sorted_pair')['source'].unique()\n",
    "duplicates_across_sets = duplicates_across_sets[duplicates_across_sets.apply(lambda x: len(x) > 1)]\n",
    "print(f'Found {len(duplicates_across_sets)} duplicate records between the training and test sets')\n",
    "\n",
    "# Remove duplicates within the training and testing datasets separately.\n",
    "# For the training dataset:\n",
    "train_df_clean['sorted_pair'] = train_df_clean.apply(\n",
    "    lambda row: tuple(sorted([row['text'], row['text_b']])), axis=1\n",
    ")\n",
    "train_df_clean = train_df_clean.drop_duplicates(subset='sorted_pair', keep='first')\n",
    "print(f'Removed duplicates from the training dataset.')\n",
    "\n",
    "# For the testing dataset:\n",
    "test_df_clean['sorted_pair'] = test_df_clean.apply(\n",
    "    lambda row: tuple(sorted([row['text'], row['text_b']])), axis=1\n",
    ")\n",
    "test_df_clean = test_df_clean.drop_duplicates(subset='sorted_pair', keep='first')\n",
    "print(f'Removed duplicates from the testing dataset.')\n",
    "\n",
    "# Convert the datasets to a list of tuples in the format (text, text_b, label).\n",
    "train_dataset = list(train_df_clean[['text', 'text_b', 'label']].itertuples(index=False, name=None))\n",
    "test_dataset = list(test_df_clean[['text', 'text_b', 'label']].itertuples(index=False, name=None))\n",
    "\n",
    "print(f'Final len of train dataset: {len(train_dataset)}')\n",
    "print(f'Final len of test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Save the datasets to JSON files.\n",
    "with open('../data/train_dataset.json', 'w') as file:\n",
    "    json.dump(train_dataset, file)\n",
    "\n",
    "with open('../data/test_dataset.json', 'w') as file:\n",
    "    json.dump(test_dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a444d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
